---
title: "Slides prepared for ABN"
author: "Estevao Alvarenga"
date: "16/11/2021"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# load libraries
library(ggplot2)
library(dplyr)
```

## Introduction

The project outlined here describes the high level design of a new digital service on the sustainable urbanisation business.

The service predicts short-term (up to 7 days) energy demand for industrial business, based on the building characteristics, inclusing usage, and weather forecasts.

The service will be consumed by our customers through an API. We might also consider implementing a web-based UI, depending on customer feedback.

## Data exploration: metadata

```{r metadataExpl}
dataFolder <- 'receivedData_DO_NOT_EDIT/'
buildingMetadata <- read.csv(paste0(dataFolder, 'building_metadata.csv'))

ggplot(buildingMetadata, aes(x = year_built, y = square_feet)) +
  geom_point(aes(colour = floor_count)) +
  facet_wrap(~primary_use)
```

Notes:
- Missing data points, either a flexible model or imputation will be required
- Unbalanced dataset

Possible imputation:
1. Bin square_feet per primary_use, interpolate floor year_built
2. Bin square_feet and year_built per primary_use, interpolate floor_count
3. Could interpolate from energy usage as well, but not if that will be the predictor

## Data exploration: meter readings

```{r meterReadExpl}
buildingMeterReadings <- read.csv(paste0(dataFolder, 'building_meter_readings.csv'))

# consolidating meter readings on same building
buildingMeterReadingsConsolidated <- buildingMeterReadings %>%
  group_by(building_id, timestamp) %>%
  summarise(meter_multi_reading = sum(meter_reading), .groups = 'drop') %>%
  mutate(timestamp = as.POSIXct(timestamp))

# plot of sample of buildings
buildingMeterReadingsConsolidated %>%
  filter(building_id %in% 1:16) %>%
  ggplot(aes(x = timestamp, y = meter_multi_reading)) +
  geom_line(aes(colour = building_id)) +
  facet_wrap(~building_id, scales = 'free') +
  theme(legend.title = element_blank())
```

Notes:
- No missing data on current dataset
- However we still need to protect against missing data for production
- Scales varies significantly, a model that can operate on these conditions is required

Possible augumentation and inputation:
- Past consumption of own building and aggregate neigbourhood (sites) should be evaluated as it could be a good predictor
- Use [STL decomposition](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/stl) and input using the interpolation of the seasonal + trend signals
- Use ARIMA to interpolate values (possibly leveraging a [Kalman Filter (KF) approach](https://stats.stackexchange.com/questions/104565/how-to-use-auto-arima-to-impute-missing-values))

## Data exploration: weather data

```{r weatherExpl}
weatherData <- read.csv(paste0(dataFolder, 'weather_data.csv'))
weatherData$timestamp <- as.POSIXct(weatherData$timestamp)
```

Notes:
- Missing data points

Possible augmentation and data imputation:
- Generally weather data can be augmented with engineering variables, lags and moving averages for increased prediction power
- Similar to the meter data, STL decomposition or ARIMA + KF could be used for imputation
- Alternatively, if we had x,y coordinates for the site_id's we could also perform krigging with neighbour sites


## Modelling process

Data pre-processing (all steps substantiated with visualisations)

1. All variables must be converted into their natural types (integer, numeric, categorical, timestamp)
2. Imputation is performed - good practice for production applications
3. Variable engineering is performed (such as new calculated variables) in consultation with subject matter experts (SME)
4. Datasets are joined, based on common ids

Model development

5. Train/test split is performed (walk-forward cross validation)
6. Variable importance is assessed (e.g. Boruta algorithm)
7. Model development takes into account out-of-sample (OOS) metrics for benchmarking
8. After model is selected, investigation plots are built (e.g. partial dependence plots, variable importance, etc) and these are discussed with SME
9. Model OOS metrics threshold is defined with SME, which will be used for continued retraining and evaluation of automated future model quality

## Productisation of solution
- A team with the below skills will be formed (not necessarily one person per skill):
   - Business / SME
   - Data science
   - MLOps (CICD pipelines)
   - IT Architecture and Information Risk Management
- Use Azure Machine Learning (AzureML) as the bank currently has its data on Azure and AzureML offers a simple data science solution with productisation features
   - We will use a development, test and production environment
- Data will be sourced appropriately and stored as dataset connections on Azure, without interim storage
   - Since only models will be stored, information risk management controls can be simpler than general cases
- The data process and model development process will be converted into a CICD pipeline:
   - Data tests will be defined and implemented, so that the process only continues if previous tests have been successful
   - Chosen model metrics will be compared with defined threshold, as a final test
   - Successful models will be converted into artefact for deployment
- Deployed artefacts will be exposed as endpoints on AzureML in the test environment, enabling the service to be used as an API (for test)
- Service is tested with several test datasets (engineered to break functionalities) in an automated fashion, and results are collected
   - Data scientist will look at test results and, if necessary, run manual tests
   - When satisfied, the CICD pipeline is going to be manually engaged to move the current model to the production environment
   
## Further service opportunities

- With enough data gathered, by accessing energy price data, the service could be extended to predict local energy price
- Additonal short term service: With energy price prediction, the service could send signal to automatically dispatch extra cooling/heating, for oprimising the total energy cost
- Additional long term service: With long term data collected it is possible to suggest battery stack investments based on minimisation of energy costs in presence of storage
